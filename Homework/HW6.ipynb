{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7514c3-a4e7-4f0f-9cd6-a6f55383b2e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe1528b-d965-4171-882a-7efdd813fd70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2260ea12-9deb-4cd7-9413-3fb9dea7828d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1054</td>\n",
       "      <td>54.70</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.027514</td>\n",
       "      <td>0.051898</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>108</td>\n",
       "      <td>27.36</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1516</td>\n",
       "      <td>62.16</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.041003</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1791</td>\n",
       "      <td>92.31</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016192</td>\n",
       "      <td>0.051541</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>430</td>\n",
       "      <td>81.53</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.062791</td>\n",
       "      <td>0.189605</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "0           5                    1054       54.70              7   \n",
       "1           3                     108       27.36              5   \n",
       "2           3                    1516       62.16              3   \n",
       "3           6                    1791       92.31              8   \n",
       "4           5                     430       81.53              3   \n",
       "\n",
       "   scansWithoutRegistration  quantityModifications  scannedLineItemsPerSecond  \\\n",
       "0                         0                      3                   0.027514   \n",
       "1                         2                      4                   0.129630   \n",
       "2                        10                      5                   0.008575   \n",
       "3                         4                      4                   0.016192   \n",
       "4                         7                      2                   0.062791   \n",
       "\n",
       "   valuePerSecond  lineItemVoidsPerPosition  fraud  \n",
       "0        0.051898                  0.241379      0  \n",
       "1        0.253333                  0.357143      0  \n",
       "2        0.041003                  0.230769      0  \n",
       "3        0.051541                  0.275862      0  \n",
       "4        0.189605                  0.111111      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd; pd.set_option('display.max_columns', 100)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import recall_score, classification_report, confusion_matrix, make_scorer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.feature_selection import RFE\n",
    "from cost_function import cost_function, cost_function_cutoff\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'grant-gonnerman-data-445'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "file_key = 'train.csv'\n",
    "\n",
    "bucket_object = bucket.Object(file_key)\n",
    "file_object = bucket_object.get()\n",
    "file_content_stream = file_object.get('Body')\n",
    "\n",
    "file_key2 = 'test.csv'\n",
    "\n",
    "bucket_object2 = bucket.Object(file_key2)\n",
    "file_object2 = bucket_object2.get()\n",
    "file_content_stream2 = file_object2.get('Body')\n",
    "\n",
    "# reading data file\n",
    "train = pd.read_csv(file_content_stream, delimiter = '|')\n",
    "test = pd.read_csv(file_content_stream2, delimiter = '|')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd37790-6082-4bb5-940e-e751025618a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5: trustLevel, totalScanTimeInSeconds, interation_1, heredity_1, heridity_2\n",
    "# top 6: trustLevel, totalScanTimeInSeconds, interation_1, heredity_1, heridity_2, DT_1\n",
    "# top 7: trustLevel, totalScanTimeInSeconds, interation_1, heredity_1, heridity_2, scansWithoutRegistration, DT_1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc11716-50fb-4683-b359-fb09141c85be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['interaction_1'] = train['totalScanTimeInSeconds'] * train['scannedLineItemsPerSecond']\n",
    "train['heredity_1'] = train['trustLevel'] * train['interaction_1']\n",
    "train['heredity_2'] = train['trustLevel'] * train['scannedLineItemsPerSecond']\n",
    "train['DT_1'] = np.where((train['trustLevel'] <= 1.5) & (train['heredity_1'] <= 4.412) & (train['totalScanTimeInSeconds'] <= 281.406), 1, 0)\n",
    "\n",
    "test['interaction_1'] = test['totalScanTimeInSeconds'] * test['scannedLineItemsPerSecond']\n",
    "test['heredity_1'] = test['trustLevel'] * test['interaction_1']\n",
    "test['heredity_2'] = test['trustLevel'] * test['scannedLineItemsPerSecond']\n",
    "test['DT_1'] = np.where((test['trustLevel'] <= 1.5) & (test['heredity_1'] <= 4.412) & (test['totalScanTimeInSeconds'] <= 281.406), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0fd11d-6710-4cec-992d-21641d63be96",
   "metadata": {},
   "source": [
    "# GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0f04840-0c2e-4f99-b84e-64a3bbd7c047",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[347   8]\n",
      " [  0  21]]\n",
      "the cost of the rf model is:  65\n"
     ]
    }
   ],
   "source": [
    "# defining imput and target\n",
    "x1 = train[['trustLevel', 'totalScanTimeInSeconds', 'interaction_1', 'heredity_1', 'heredity_2']]\n",
    "y1 = train['fraud']\n",
    "\n",
    "x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(x1, y1, test_size = 0.2, stratify = y1)\n",
    "\n",
    "# grid search cv\n",
    "RF_param_grid = {'n_estimators': [100, 300, 500],\n",
    "                 'min_samples_split': [10, 15],\n",
    "                 'min_samples_leaf': [5, 7],\n",
    "                 'max_depth': [3, 5, 7]}\n",
    "\n",
    "# defining custom scorer\n",
    "my_scorer_function = make_scorer(cost_function, greater_is_better = True, needs_proba = True)\n",
    "\n",
    "# running grid search \n",
    "rf_grid1 = GridSearchCV(estimator = RandomForestClassifier(), param_grid = RF_param_grid, cv = 3, scoring = my_scorer_function, n_jobs = -1).fit(x_train_1, y_train_1)\n",
    "\n",
    "# extracting best hyper params\n",
    "rf_grid1.best_params_\n",
    "\n",
    "# running model with best parameters\n",
    "rf_opt1 = RandomForestClassifier(**rf_grid1.best_params_).fit(x_train_1, y_train_1)\n",
    "\n",
    "# predicting on test\n",
    "rf_pred1 = rf_opt1.predict_proba(x_test_1)[:,1]\n",
    "\n",
    "# getting optimal cutoff\n",
    "opt_cutoff1 = cost_function_cutoff(y_test_1, rf_pred1)\n",
    "\n",
    "# likilyhoods to labels\n",
    "rf_label1 = np.where(rf_pred1 < opt_cutoff1, 0, 1)\n",
    "\n",
    "# scoring the model \n",
    "con_mat = confusion_matrix(y_test_1, rf_label1)\n",
    "print(con_mat)\n",
    "print('the cost of the rf model is: ', -25 * con_mat[1, 0] - 5 * con_mat[0, 1] + 5 * con_mat[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2725290-61be-4c00-9761-505528baee9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[341  14]\n",
      " [  1  20]]\n",
      "the cost of the rf model is:  5\n"
     ]
    }
   ],
   "source": [
    "# defining imput and target\n",
    "x2 = train[['trustLevel', 'totalScanTimeInSeconds', 'interaction_1', 'heredity_1', 'heredity_2', 'DT_1']]\n",
    "y2 = train['fraud']\n",
    "\n",
    "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(x2, y2, test_size = 0.2, stratify = y2)\n",
    "\n",
    "# running grid search \n",
    "rf_grid2 = GridSearchCV(estimator = RandomForestClassifier(), param_grid = RF_param_grid, cv = 3, scoring = my_scorer_function, n_jobs = -1).fit(x_train_2, y_train_2)\n",
    "\n",
    "# extracting best hyper params\n",
    "rf_grid2.best_params_\n",
    "\n",
    "# running model with best parameters\n",
    "rf_opt2 = RandomForestClassifier(**rf_grid2.best_params_).fit(x_train_2, y_train_2)\n",
    "\n",
    "# predicting on test\n",
    "rf_pred2 = rf_opt2.predict_proba(x_test_2)[:,1]\n",
    "\n",
    "# getting optimal cutoff\n",
    "opt_cutoff2 = cost_function_cutoff(y_test_2, rf_pred2)\n",
    "\n",
    "# likilyhoods to labels\n",
    "rf_label2 = np.where(rf_pred2 < opt_cutoff2, 0, 1)\n",
    "\n",
    "# scoring the model \n",
    "con_mat = confusion_matrix(y_test_2, rf_label2)\n",
    "print(con_mat)\n",
    "print('the cost of the rf model is: ', -25 * con_mat[1, 0] - 5 * con_mat[0, 1] + 5 * con_mat[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a975eceb-2357-4e0b-a61c-ec33890ebfbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[349   6]\n",
      " [  0  21]]\n",
      "the cost of the rf model is:  75\n"
     ]
    }
   ],
   "source": [
    "# defining imput and target\n",
    "x3 = train[['trustLevel', 'totalScanTimeInSeconds', 'interaction_1', 'heredity_1', 'heredity_2', 'DT_1', 'scansWithoutRegistration']]\n",
    "y3 = train['fraud']\n",
    "\n",
    "x_train_3, x_test_3, y_train_3, y_test_3 = train_test_split(x3, y3, test_size = 0.2, stratify = y3)\n",
    "\n",
    "# running grid search \n",
    "rf_grid3 = GridSearchCV(estimator = RandomForestClassifier(), param_grid = RF_param_grid, cv = 3, scoring = my_scorer_function, n_jobs = -1).fit(x_train_3, y_train_3)\n",
    "\n",
    "# extracting best hyper params\n",
    "rf_grid3.best_params_\n",
    "\n",
    "# running model with best parameters\n",
    "rf_opt3 = RandomForestClassifier(**rf_grid3.best_params_).fit(x_train_3, y_train_3)\n",
    "\n",
    "# predicting on test\n",
    "rf_pred3 = rf_opt3.predict_proba(x_test_3)[:,1]\n",
    "\n",
    "# getting optimal cutoff\n",
    "opt_cutoff3 = cost_function_cutoff(y_test_3, rf_pred3)\n",
    "\n",
    "# likilyhoods to labels\n",
    "rf_label3 = np.where(rf_pred3 < opt_cutoff3, 0, 1)\n",
    "\n",
    "# scoring the model \n",
    "con_mat = confusion_matrix(y_test_3, rf_label3)\n",
    "print(con_mat)\n",
    "print('the cost of the rf model is: ', -25 * con_mat[1, 0] - 5 * con_mat[0, 1] + 5 * con_mat[1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae7448d-6fc4-4a7e-acff-4f0b2541d72b",
   "metadata": {},
   "source": [
    "# RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ad13d37-6145-4f3e-9f39-02fde47929e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[347   8]\n",
      " [  0  21]]\n",
      "the cost of the gb model is:  65\n"
     ]
    }
   ],
   "source": [
    "Gb_param_grid = {'n_estimators': [100, 300],\n",
    "                    'min_samples_split': [10, 15],\n",
    "                    'min_samples_leaf': [5, 7],\n",
    "                    'max_depth': [3, 5, 7],\n",
    "                    'learning_rate': [0.01]}\n",
    "\n",
    "# running grid search \n",
    "gb_grid1 = RandomizedSearchCV(estimator = GradientBoostingClassifier(), \n",
    "                               param_distributions = Gb_param_grid, cv = 3, scoring = my_scorer_function, n_jobs = -1).fit(x_train_1, y_train_1)\n",
    "\n",
    "# extracting best hyper params\n",
    "gb_grid1.best_params_\n",
    "\n",
    "# running model with best parameters\n",
    "gb_opt1 = GradientBoostingClassifier(**gb_grid1.best_params_).fit(x_train_1, y_train_1)\n",
    "\n",
    "# predicting on test\n",
    "gb_pred1 = gb_opt1.predict_proba(x_test_1)[:,1]\n",
    "\n",
    "# getting optimal cutoff\n",
    "opt_cutoff1 = cost_function_cutoff(y_test_1, gb_pred1)\n",
    "\n",
    "# likilyhoods to labels\n",
    "gb_label1 = np.where(gb_pred1 < opt_cutoff1, 0, 1)\n",
    "\n",
    "# scoring the model \n",
    "con_mat = confusion_matrix(y_test_1, gb_label1)\n",
    "print(con_mat)\n",
    "print('the cost of the gb model is: ', -25 * con_mat[1, 0] - 5 * con_mat[0, 1] + 5 * con_mat[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8222e126-6dd3-4f0c-9236-3bf24e126331",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[346   9]\n",
      " [  4  17]]\n",
      "the cost of the gb model is:  -60\n"
     ]
    }
   ],
   "source": [
    "# running grid search \n",
    "gb_grid2 = RandomizedSearchCV(estimator = GradientBoostingClassifier(), \n",
    "                               param_distributions = Gb_param_grid, cv = 3, scoring = my_scorer_function, n_jobs = -1).fit(x_train_2, y_train_2)\n",
    "\n",
    "# extracting best hyper params\n",
    "gb_grid2.best_params_\n",
    "\n",
    "# running model with best parameters\n",
    "gb_opt2 = GradientBoostingClassifier(**gb_grid2.best_params_).fit(x_train_2, y_train_2)\n",
    "\n",
    "# predicting on test\n",
    "gb_pred2 = gb_opt2.predict_proba(x_test_2)[:,1]\n",
    "\n",
    "# getting optimal cutoff\n",
    "opt_cutoff2 = cost_function_cutoff(y_test_2, gb_pred2)\n",
    "\n",
    "# likilyhoods to labels\n",
    "gb_label2 = np.where(gb_pred2 < opt_cutoff2, 0, 1)\n",
    "\n",
    "# scoring the model \n",
    "con_mat = confusion_matrix(y_test_2, gb_label2)\n",
    "print(con_mat)\n",
    "print('the cost of the gb model is: ', -25 * con_mat[1, 0] - 5 * con_mat[0, 1] + 5 * con_mat[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dae8408-0414-46a2-a092-87332483c443",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[348   7]\n",
      " [  0  21]]\n",
      "the cost of the gb model is:  70\n"
     ]
    }
   ],
   "source": [
    "# running grid search \n",
    "gb_grid3 = RandomizedSearchCV(estimator = GradientBoostingClassifier(), \n",
    "                               param_distributions = Gb_param_grid, cv = 3, scoring = my_scorer_function, n_jobs = -1).fit(x_train_3, y_train_3)\n",
    "\n",
    "# extracting best hyper params\n",
    "gb_grid3.best_params_\n",
    "\n",
    "# running model with best parameters\n",
    "gb_opt3 = GradientBoostingClassifier(**gb_grid3.best_params_).fit(x_train_3, y_train_3)\n",
    "\n",
    "# predicting on test\n",
    "gb_pred3 = gb_opt3.predict_proba(x_test_3)[:,1]\n",
    "\n",
    "# getting optimal cutoff\n",
    "opt_cutoff3 = cost_function_cutoff(y_test_3, gb_pred3)\n",
    "\n",
    "# likilyhoods to labels\n",
    "gb_label3 = np.where(gb_pred3 < opt_cutoff3, 0, 1)\n",
    "\n",
    "# scoring the model \n",
    "con_mat = confusion_matrix(y_test_3, gb_label3)\n",
    "print(con_mat)\n",
    "print('the cost of the gb model is: ', -25 * con_mat[1, 0] - 5 * con_mat[0, 1] + 5 * con_mat[1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1262966-57af-4d6e-bca0-cb35d38a8913",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bc47f12-37d1-4f4c-be09-9bf715dde2d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-31 20:36:23,509]\u001b[0m A new study created in memory with name: no-name-2714ff73-0c30-4d13-a94a-a3776d407137\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:24,625]\u001b[0m Trial 0 finished with value: 6.666666666666667 and parameters: {'n_estimators': 365, 'max_depth': 5, 'min_child_weight': 7, 'learning_rate': 0.08799433405250016, 'gamma': 0.1597397219101332, 'subsample': 0.9561336306591741}. Best is trial 0 with value: 6.666666666666667.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:25,410]\u001b[0m Trial 1 finished with value: 13.333333333333334 and parameters: {'n_estimators': 458, 'max_depth': 7, 'min_child_weight': 6, 'learning_rate': 0.015552765108463137, 'gamma': 0.1137143155567222, 'subsample': 0.8228238086975045}. Best is trial 1 with value: 13.333333333333334.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:26,186]\u001b[0m Trial 2 finished with value: 8.333333333333334 and parameters: {'n_estimators': 420, 'max_depth': 7, 'min_child_weight': 6, 'learning_rate': 0.02557051251937307, 'gamma': 0.1224374927110069, 'subsample': 0.9064461448602504}. Best is trial 1 with value: 13.333333333333334.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:26,732]\u001b[0m Trial 3 finished with value: 5.0 and parameters: {'n_estimators': 352, 'max_depth': 3, 'min_child_weight': 7, 'learning_rate': 0.0893447014228488, 'gamma': 0.11228176128845138, 'subsample': 0.8108757025199168}. Best is trial 1 with value: 13.333333333333334.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:27,442]\u001b[0m Trial 4 finished with value: 18.333333333333332 and parameters: {'n_estimators': 469, 'max_depth': 7, 'min_child_weight': 6, 'learning_rate': 0.03770858244680119, 'gamma': 0.1731794890573506, 'subsample': 0.9955852873393051}. Best is trial 4 with value: 18.333333333333332.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:28,044]\u001b[0m Trial 5 finished with value: 8.333333333333334 and parameters: {'n_estimators': 311, 'max_depth': 7, 'min_child_weight': 6, 'learning_rate': 0.017226017916205177, 'gamma': 0.2319662201156983, 'subsample': 0.9066300670626525}. Best is trial 4 with value: 18.333333333333332.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:28,740]\u001b[0m Trial 6 finished with value: 8.333333333333334 and parameters: {'n_estimators': 491, 'max_depth': 7, 'min_child_weight': 7, 'learning_rate': 0.07470350468803325, 'gamma': 0.261366869527799, 'subsample': 0.8397491115206761}. Best is trial 4 with value: 18.333333333333332.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:29,422]\u001b[0m Trial 7 finished with value: 18.333333333333332 and parameters: {'n_estimators': 480, 'max_depth': 4, 'min_child_weight': 6, 'learning_rate': 0.08599530317514044, 'gamma': 0.25301578249125223, 'subsample': 0.9954581581647357}. Best is trial 4 with value: 18.333333333333332.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:30,008]\u001b[0m Trial 8 finished with value: 15.0 and parameters: {'n_estimators': 357, 'max_depth': 7, 'min_child_weight': 5, 'learning_rate': 0.04448310934063425, 'gamma': 0.1418220191384531, 'subsample': 0.8096505160782942}. Best is trial 4 with value: 18.333333333333332.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:30,567]\u001b[0m Trial 9 finished with value: 15.0 and parameters: {'n_estimators': 318, 'max_depth': 6, 'min_child_weight': 6, 'learning_rate': 0.04770008940308465, 'gamma': 0.23065242991664586, 'subsample': 0.953287700459798}. Best is trial 4 with value: 18.333333333333332.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:31,285]\u001b[0m Trial 10 finished with value: 10.0 and parameters: {'n_estimators': 431, 'max_depth': 5, 'min_child_weight': 5, 'learning_rate': 0.06132711681955002, 'gamma': 0.1756686145415199, 'subsample': 0.9906146680711134}. Best is trial 4 with value: 18.333333333333332.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:32,085]\u001b[0m Trial 11 finished with value: 8.333333333333334 and parameters: {'n_estimators': 491, 'max_depth': 3, 'min_child_weight': 5, 'learning_rate': 0.09996372937545842, 'gamma': 0.2968617779448107, 'subsample': 0.9985277513318287}. Best is trial 4 with value: 18.333333333333332.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:32,792]\u001b[0m Trial 12 finished with value: 15.0 and parameters: {'n_estimators': 458, 'max_depth': 4, 'min_child_weight': 6, 'learning_rate': 0.06404657505056084, 'gamma': 0.19366375910081204, 'subsample': 0.9665875833082804}. Best is trial 4 with value: 18.333333333333332.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:33,537]\u001b[0m Trial 13 finished with value: 15.0 and parameters: {'n_estimators': 460, 'max_depth': 4, 'min_child_weight': 6, 'learning_rate': 0.03612204432011015, 'gamma': 0.2077415289864837, 'subsample': 0.9986878253884591}. Best is trial 4 with value: 18.333333333333332.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:34,493]\u001b[0m Trial 14 finished with value: 18.333333333333332 and parameters: {'n_estimators': 402, 'max_depth': 4, 'min_child_weight': 5, 'learning_rate': 0.05522014415652576, 'gamma': 0.17314683533934044, 'subsample': 0.9371946755901643}. Best is trial 4 with value: 18.333333333333332.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:35,867]\u001b[0m Trial 15 finished with value: 1.6666666666666667 and parameters: {'n_estimators': 492, 'max_depth': 6, 'min_child_weight': 7, 'learning_rate': 0.07247570794187173, 'gamma': 0.14717046225278496, 'subsample': 0.9692968985113374}. Best is trial 4 with value: 18.333333333333332.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:37,345]\u001b[0m Trial 16 finished with value: 15.0 and parameters: {'n_estimators': 438, 'max_depth': 6, 'min_child_weight': 6, 'learning_rate': 0.03355329588172434, 'gamma': 0.20462898058262505, 'subsample': 0.9362909775267814}. Best is trial 4 with value: 18.333333333333332.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:38,713]\u001b[0m Trial 17 finished with value: 5.0 and parameters: {'n_estimators': 473, 'max_depth': 4, 'min_child_weight': 7, 'learning_rate': 0.044555299131774924, 'gamma': 0.262630011179431, 'subsample': 0.979668786315874}. Best is trial 4 with value: 18.333333333333332.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:40,203]\u001b[0m Trial 18 finished with value: 5.0 and parameters: {'n_estimators': 395, 'max_depth': 5, 'min_child_weight': 6, 'learning_rate': 0.011238708415970917, 'gamma': 0.18715997187911665, 'subsample': 0.9791779548927037}. Best is trial 4 with value: 18.333333333333332.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:41,373]\u001b[0m Trial 19 finished with value: 16.666666666666668 and parameters: {'n_estimators': 472, 'max_depth': 3, 'min_child_weight': 5, 'learning_rate': 0.030373049116724407, 'gamma': 0.22188657170900647, 'subsample': 0.8745064866051602}. Best is trial 4 with value: 18.333333333333332.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[347   8]\n",
      " [  0  21]]\n",
      "the cost of the xgb model is:  65\n"
     ]
    }
   ],
   "source": [
    "class objective:\n",
    "    \n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "        \n",
    "    def __call__(self, trial):\n",
    "        params = dict(n_estimators = trial.suggest_int('n_estimators', 300, 500),\n",
    "                     max_depth = trial.suggest_int('max_depth', 3, 7),\n",
    "                     min_child_weight = trial.suggest_int('min_child_weight', 5, 7),\n",
    "                     learning_rate= trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "                     gamma= trial.suggest_float('gamma', 0.1, 0.3),\n",
    "                     subsample= trial.suggest_float('subsample', 0.8, 1))\n",
    "        scores = list()\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = self.seed)\n",
    "        \n",
    "        for train_idx, test_idx in skf.split(x1, y1):\n",
    "            \n",
    "            ## Splitting the data \n",
    "            x_train, x_test = x1.iloc[train_idx], x1.iloc[test_idx]\n",
    "            y_train, y_test = y1.iloc[train_idx], y1.iloc[test_idx]\n",
    "            \n",
    "            xgb_md = XGBClassifier(**params).fit(x_train, y_train)\n",
    "            \n",
    "            xgb_pred = xgb_md.predict_proba(x_test)[:,1]\n",
    "            \n",
    "            score = cost_function(y_test, xgb_pred)\n",
    "            scores.append(score)\n",
    "        \n",
    "        return np.mean(scores)\n",
    "\n",
    "seed = 42\n",
    "n_trials = 20\n",
    "\n",
    "study_meta = optuna.create_study(direction = 'maximize')\n",
    "study_meta.optimize(objective(seed), n_trials = n_trials)\n",
    "study_meta.best_trial.params\n",
    "\n",
    "# running model with best parameters\n",
    "xgb_opt1 = XGBClassifier(**study_meta.best_trial.params).fit(x_train_1, y_train_1)\n",
    "\n",
    "# predicting on test\n",
    "xgb_pred1 = xgb_opt1.predict_proba(x_test_1)[:,1]\n",
    "\n",
    "# getting optimal cutoff\n",
    "opt_cutoff1 = cost_function_cutoff(y_test_1, xgb_pred1)\n",
    "\n",
    "# likilyhoods to labels\n",
    "xgb_label1 = np.where(xgb_pred1 < opt_cutoff1, 0, 1)\n",
    "\n",
    "# scoring the model \n",
    "con_mat = confusion_matrix(y_test_1, xgb_label1)\n",
    "print(con_mat)\n",
    "print('the cost of the xgb model is: ', -25 * con_mat[1, 0] - 5 * con_mat[0, 1] + 5 * con_mat[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfc2d7e0-119a-4d6f-bd10-26dd35062960",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-31 20:34:06,456]\u001b[0m A new study created in memory with name: no-name-a45ac854-88cc-4a11-8d0e-3621992f858d\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:34:07,125]\u001b[0m Trial 0 finished with value: 20.0 and parameters: {'n_estimators': 383, 'max_depth': 5, 'min_child_weight': 7, 'learning_rate': 0.02097612352106601, 'gamma': 0.238325848458318, 'subsample': 0.9500757864263396}. Best is trial 0 with value: 20.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:34:07,752]\u001b[0m Trial 1 finished with value: 16.666666666666668 and parameters: {'n_estimators': 407, 'max_depth': 4, 'min_child_weight': 6, 'learning_rate': 0.046333008663413756, 'gamma': 0.2326492246395072, 'subsample': 0.9295858824762935}. Best is trial 0 with value: 20.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:34:08,369]\u001b[0m Trial 2 finished with value: 13.333333333333334 and parameters: {'n_estimators': 369, 'max_depth': 7, 'min_child_weight': 5, 'learning_rate': 0.04867347135815458, 'gamma': 0.21029846143782577, 'subsample': 0.9987594611050464}. Best is trial 0 with value: 20.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:34:09,012]\u001b[0m Trial 3 finished with value: 11.666666666666666 and parameters: {'n_estimators': 417, 'max_depth': 4, 'min_child_weight': 5, 'learning_rate': 0.05291762591780056, 'gamma': 0.1331664386218628, 'subsample': 0.860985581193198}. Best is trial 0 with value: 20.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:34:09,568]\u001b[0m Trial 4 finished with value: 16.666666666666668 and parameters: {'n_estimators': 302, 'max_depth': 5, 'min_child_weight': 5, 'learning_rate': 0.035257482923608974, 'gamma': 0.11499550731096253, 'subsample': 0.897179806118717}. Best is trial 0 with value: 20.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:34:10,188]\u001b[0m Trial 5 finished with value: 11.666666666666666 and parameters: {'n_estimators': 391, 'max_depth': 5, 'min_child_weight': 6, 'learning_rate': 0.04088628428332405, 'gamma': 0.18282414541187547, 'subsample': 0.8704502700373293}. Best is trial 0 with value: 20.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:34:10,919]\u001b[0m Trial 6 finished with value: 18.333333333333332 and parameters: {'n_estimators': 484, 'max_depth': 6, 'min_child_weight': 6, 'learning_rate': 0.03648738075919463, 'gamma': 0.29124744828604, 'subsample': 0.9767695422354308}. Best is trial 0 with value: 20.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:34:11,504]\u001b[0m Trial 7 finished with value: 6.666666666666667 and parameters: {'n_estimators': 362, 'max_depth': 4, 'min_child_weight': 5, 'learning_rate': 0.06589645360028812, 'gamma': 0.2366435876510059, 'subsample': 0.9736378176143448}. Best is trial 0 with value: 20.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:34:12,217]\u001b[0m Trial 8 finished with value: 10.0 and parameters: {'n_estimators': 500, 'max_depth': 3, 'min_child_weight': 5, 'learning_rate': 0.060024168152330803, 'gamma': 0.28254187616924636, 'subsample': 0.8841205863015262}. Best is trial 0 with value: 20.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:34:12,842]\u001b[0m Trial 9 finished with value: 10.0 and parameters: {'n_estimators': 419, 'max_depth': 3, 'min_child_weight': 5, 'learning_rate': 0.07432941508473047, 'gamma': 0.23402182950540432, 'subsample': 0.8644975048797783}. Best is trial 0 with value: 20.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:34:13,510]\u001b[0m Trial 10 finished with value: 13.333333333333334 and parameters: {'n_estimators': 324, 'max_depth': 7, 'min_child_weight': 7, 'learning_rate': 0.010602217151801402, 'gamma': 0.16882532091717387, 'subsample': 0.8021155370136513}. Best is trial 0 with value: 20.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:34:14,226]\u001b[0m Trial 11 finished with value: 1.6666666666666667 and parameters: {'n_estimators': 475, 'max_depth': 6, 'min_child_weight': 7, 'learning_rate': 0.09513072904497749, 'gamma': 0.2916200614906126, 'subsample': 0.9478773589694365}. Best is trial 0 with value: 20.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:34:15,005]\u001b[0m Trial 12 finished with value: 6.666666666666667 and parameters: {'n_estimators': 464, 'max_depth': 6, 'min_child_weight': 7, 'learning_rate': 0.025802579860652767, 'gamma': 0.2702895361440362, 'subsample': 0.9454974669540215}. Best is trial 0 with value: 20.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:34:15,789]\u001b[0m Trial 13 finished with value: 15.0 and parameters: {'n_estimators': 443, 'max_depth': 6, 'min_child_weight': 6, 'learning_rate': 0.023269860861238115, 'gamma': 0.2987080456555954, 'subsample': 0.9958608172936672}. Best is trial 0 with value: 20.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:34:16,501]\u001b[0m Trial 14 finished with value: 8.333333333333334 and parameters: {'n_estimators': 345, 'max_depth': 6, 'min_child_weight': 7, 'learning_rate': 0.011458301123972102, 'gamma': 0.2631580099472139, 'subsample': 0.9684110948007354}. Best is trial 0 with value: 20.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:34:17,243]\u001b[0m Trial 15 finished with value: 15.0 and parameters: {'n_estimators': 446, 'max_depth': 5, 'min_child_weight': 6, 'learning_rate': 0.029829600803829424, 'gamma': 0.2592896779794787, 'subsample': 0.9220304725859736}. Best is trial 0 with value: 20.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:34:17,920]\u001b[0m Trial 16 finished with value: 18.333333333333332 and parameters: {'n_estimators': 386, 'max_depth': 7, 'min_child_weight': 6, 'learning_rate': 0.03633875003925842, 'gamma': 0.29950462368059455, 'subsample': 0.9708993276078786}. Best is trial 0 with value: 20.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:34:18,740]\u001b[0m Trial 17 finished with value: 8.333333333333334 and parameters: {'n_estimators': 495, 'max_depth': 6, 'min_child_weight': 7, 'learning_rate': 0.020867151196349423, 'gamma': 0.25971542615565024, 'subsample': 0.9521090217325743}. Best is trial 0 with value: 20.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:34:19,455]\u001b[0m Trial 18 finished with value: 15.0 and parameters: {'n_estimators': 437, 'max_depth': 5, 'min_child_weight': 6, 'learning_rate': 0.03444959444738478, 'gamma': 0.21551217433454925, 'subsample': 0.9171658907508794}. Best is trial 0 with value: 20.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:34:20,132]\u001b[0m Trial 19 finished with value: 8.333333333333334 and parameters: {'n_estimators': 344, 'max_depth': 5, 'min_child_weight': 6, 'learning_rate': 0.018246162759092563, 'gamma': 0.27830452760887725, 'subsample': 0.9820716778808186}. Best is trial 0 with value: 20.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[344  11]\n",
      " [  1  20]]\n",
      "the cost of the xgb model is:  20\n"
     ]
    }
   ],
   "source": [
    "class objective:\n",
    "    \n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "        \n",
    "    def __call__(self, trial):\n",
    "        params = dict(n_estimators = trial.suggest_int('n_estimators', 300, 500),\n",
    "                     max_depth = trial.suggest_int('max_depth', 3, 7),\n",
    "                     min_child_weight = trial.suggest_int('min_child_weight', 5, 7),\n",
    "                     learning_rate= trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "                     gamma= trial.suggest_float('gamma', 0.1, 0.3),\n",
    "                     subsample= trial.suggest_float('subsample', 0.8, 1))\n",
    "        scores = list()\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = self.seed)\n",
    "        \n",
    "        for train_idx, test_idx in skf.split(x2, y2):\n",
    "            \n",
    "            ## Splitting the data \n",
    "            x_train, x_test = x2.iloc[train_idx], x2.iloc[test_idx]\n",
    "            y_train, y_test = y2.iloc[train_idx], y2.iloc[test_idx]\n",
    "            \n",
    "            xgb_md = XGBClassifier(**params).fit(x_train, y_train)\n",
    "            \n",
    "            xgb_pred = xgb_md.predict_proba(x_test)[:,1]\n",
    "            \n",
    "            score = cost_function(y_test, xgb_pred)\n",
    "            scores.append(score)\n",
    "        \n",
    "        return np.mean(scores)\n",
    "\n",
    "seed = 42\n",
    "n_trials = 20\n",
    "\n",
    "study2 = optuna.create_study(direction = 'maximize')\n",
    "study2.optimize(objective(seed), n_trials = n_trials)\n",
    "study2.best_trial.params\n",
    "\n",
    "# running model with best parameters\n",
    "xgb_opt2 = XGBClassifier(**study2.best_trial.params).fit(x_train_2, y_train_2)\n",
    "\n",
    "# predicting on test\n",
    "xgb_pred2 = xgb_opt2.predict_proba(x_test_2)[:,1]\n",
    "\n",
    "# getting optimal cutoff\n",
    "opt_cutoff2 = cost_function_cutoff(y_test_2, xgb_pred2)\n",
    "\n",
    "# likilyhoods to labels\n",
    "xgb_label2 = np.where(xgb_pred2 < opt_cutoff2, 0, 1)\n",
    "\n",
    "# scoring the model \n",
    "con_mat = confusion_matrix(y_test_2, xgb_label2)\n",
    "print(con_mat)\n",
    "print('the cost of the xgb model is: ', -25 * con_mat[1, 0] - 5 * con_mat[0, 1] + 5 * con_mat[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efbac7e3-529b-4790-966d-96ee61c750b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-31 20:36:41,674]\u001b[0m A new study created in memory with name: no-name-f76a4b07-9060-4418-ac38-5a240c738313\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:42,330]\u001b[0m Trial 0 finished with value: 46.666666666666664 and parameters: {'n_estimators': 444, 'max_depth': 7, 'min_child_weight': 7, 'learning_rate': 0.09797280789474971, 'gamma': 0.19717254783308824, 'subsample': 0.8566769545266859}. Best is trial 0 with value: 46.666666666666664.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:43,055]\u001b[0m Trial 1 finished with value: 65.0 and parameters: {'n_estimators': 456, 'max_depth': 3, 'min_child_weight': 5, 'learning_rate': 0.03821288029680182, 'gamma': 0.10263846799414456, 'subsample': 0.8193509103649181}. Best is trial 1 with value: 65.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:43,568]\u001b[0m Trial 2 finished with value: 53.333333333333336 and parameters: {'n_estimators': 312, 'max_depth': 4, 'min_child_weight': 7, 'learning_rate': 0.06289536751279884, 'gamma': 0.17686234085559369, 'subsample': 0.8232796665918554}. Best is trial 1 with value: 65.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:44,201]\u001b[0m Trial 3 finished with value: 53.333333333333336 and parameters: {'n_estimators': 395, 'max_depth': 5, 'min_child_weight': 7, 'learning_rate': 0.06980094131583277, 'gamma': 0.274999900005096, 'subsample': 0.9735891657939267}. Best is trial 1 with value: 65.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:44,880]\u001b[0m Trial 4 finished with value: 56.666666666666664 and parameters: {'n_estimators': 428, 'max_depth': 7, 'min_child_weight': 7, 'learning_rate': 0.05167932708935129, 'gamma': 0.2763647325070443, 'subsample': 0.9165105251379477}. Best is trial 1 with value: 65.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:45,571]\u001b[0m Trial 5 finished with value: 55.0 and parameters: {'n_estimators': 463, 'max_depth': 3, 'min_child_weight': 7, 'learning_rate': 0.05224272580031328, 'gamma': 0.28388786846239733, 'subsample': 0.8139132911283566}. Best is trial 1 with value: 65.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:46,180]\u001b[0m Trial 6 finished with value: 56.666666666666664 and parameters: {'n_estimators': 330, 'max_depth': 7, 'min_child_weight': 7, 'learning_rate': 0.03428421483703486, 'gamma': 0.2974455637769543, 'subsample': 0.9906652871170006}. Best is trial 1 with value: 65.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:46,837]\u001b[0m Trial 7 finished with value: 60.0 and parameters: {'n_estimators': 406, 'max_depth': 3, 'min_child_weight': 7, 'learning_rate': 0.040988278670160645, 'gamma': 0.2550569400431574, 'subsample': 0.9472843443662078}. Best is trial 1 with value: 65.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:47,619]\u001b[0m Trial 8 finished with value: 45.0 and parameters: {'n_estimators': 462, 'max_depth': 3, 'min_child_weight': 7, 'learning_rate': 0.016256426269265885, 'gamma': 0.14689838964495192, 'subsample': 0.9852430421792702}. Best is trial 1 with value: 65.0.\u001b[0m\n",
      "\u001b[32m[I 2023-03-31 20:36:48,170]\u001b[0m Trial 9 finished with value: 70.0 and parameters: {'n_estimators': 323, 'max_depth': 5, 'min_child_weight': 5, 'learning_rate': 0.08199481025626429, 'gamma': 0.18013848692542023, 'subsample': 0.9112569903715746}. Best is trial 9 with value: 70.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:48,814]\u001b[0m Trial 10 finished with value: 70.0 and parameters: {'n_estimators': 355, 'max_depth': 5, 'min_child_weight': 5, 'learning_rate': 0.08302552273344191, 'gamma': 0.231604274362969, 'subsample': 0.8929946849931478}. Best is trial 9 with value: 70.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:49,431]\u001b[0m Trial 11 finished with value: 60.0 and parameters: {'n_estimators': 355, 'max_depth': 5, 'min_child_weight': 5, 'learning_rate': 0.08168558822641694, 'gamma': 0.23850145777423457, 'subsample': 0.8932764165733269}. Best is trial 9 with value: 70.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:50,075]\u001b[0m Trial 12 finished with value: 63.333333333333336 and parameters: {'n_estimators': 360, 'max_depth': 6, 'min_child_weight': 6, 'learning_rate': 0.08356702008901741, 'gamma': 0.22626759771719035, 'subsample': 0.8940395376235498}. Best is trial 9 with value: 70.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:50,690]\u001b[0m Trial 13 finished with value: 66.66666666666667 and parameters: {'n_estimators': 355, 'max_depth': 6, 'min_child_weight': 5, 'learning_rate': 0.09604840236929209, 'gamma': 0.21428455862934068, 'subsample': 0.9225759829659304}. Best is trial 9 with value: 70.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:51,276]\u001b[0m Trial 14 finished with value: 65.0 and parameters: {'n_estimators': 304, 'max_depth': 4, 'min_child_weight': 6, 'learning_rate': 0.07569248526766537, 'gamma': 0.18059257774524742, 'subsample': 0.8714841394687198}. Best is trial 9 with value: 70.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:51,928]\u001b[0m Trial 15 finished with value: 70.0 and parameters: {'n_estimators': 382, 'max_depth': 6, 'min_child_weight': 5, 'learning_rate': 0.08582813219138367, 'gamma': 0.2456368974192066, 'subsample': 0.9417537334006111}. Best is trial 9 with value: 70.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:52,515]\u001b[0m Trial 16 finished with value: 65.0 and parameters: {'n_estimators': 335, 'max_depth': 4, 'min_child_weight': 6, 'learning_rate': 0.06773630761045236, 'gamma': 0.21261966038420435, 'subsample': 0.8700091779275144}. Best is trial 9 with value: 70.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:53,276]\u001b[0m Trial 17 finished with value: 70.0 and parameters: {'n_estimators': 499, 'max_depth': 5, 'min_child_weight': 5, 'learning_rate': 0.09123991548145985, 'gamma': 0.19079545493842648, 'subsample': 0.9124230720475521}. Best is trial 9 with value: 70.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:53,862]\u001b[0m Trial 18 finished with value: 65.0 and parameters: {'n_estimators': 330, 'max_depth': 5, 'min_child_weight': 5, 'learning_rate': 0.07966359029208077, 'gamma': 0.14901123031545566, 'subsample': 0.9405839390053145}. Best is trial 9 with value: 70.0.\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:132: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages/optuna/samplers/_tpe/_erf.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  s = one / (x * x)\n",
      "\u001b[32m[I 2023-03-31 20:36:54,498]\u001b[0m Trial 19 finished with value: 50.0 and parameters: {'n_estimators': 376, 'max_depth': 6, 'min_child_weight': 6, 'learning_rate': 0.0996601014090879, 'gamma': 0.22505500863200725, 'subsample': 0.8924892342237092}. Best is trial 9 with value: 70.0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[352   3]\n",
      " [  0  21]]\n",
      "the cost of the xgb model is:  90\n"
     ]
    }
   ],
   "source": [
    "class objective:\n",
    "    \n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "        \n",
    "    def __call__(self, trial):\n",
    "        params = dict(n_estimators = trial.suggest_int('n_estimators', 300, 500),\n",
    "                     max_depth = trial.suggest_int('max_depth', 3, 7),\n",
    "                     min_child_weight = trial.suggest_int('min_child_weight', 5, 7),\n",
    "                     learning_rate= trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "                     gamma= trial.suggest_float('gamma', 0.1, 0.3),\n",
    "                     subsample= trial.suggest_float('subsample', 0.8, 1))\n",
    "        scores = list()\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = self.seed)\n",
    "        \n",
    "        for train_idx, test_idx in skf.split(x3, y3):\n",
    "            \n",
    "            ## Splitting the data \n",
    "            x_train, x_test = x3.iloc[train_idx], x3.iloc[test_idx]\n",
    "            y_train, y_test = y3.iloc[train_idx], y3.iloc[test_idx]\n",
    "            \n",
    "            xgb_md = XGBClassifier(**params).fit(x_train, y_train)\n",
    "            \n",
    "            xgb_pred = xgb_md.predict_proba(x_test)[:,1]\n",
    "            \n",
    "            score = cost_function(y_test, xgb_pred)\n",
    "            scores.append(score)\n",
    "        \n",
    "        return np.mean(scores)\n",
    "\n",
    "seed = 42\n",
    "n_trials = 20\n",
    "\n",
    "study3 = optuna.create_study(direction = 'maximize')\n",
    "study3.optimize(objective(seed), n_trials = n_trials)\n",
    "study3.best_trial.params\n",
    "\n",
    "# running model with best parameters\n",
    "xgb_opt3 = XGBClassifier(**study3.best_trial.params).fit(x_train_3, y_train_3)\n",
    "\n",
    "# predicting on test\n",
    "xgb_pred3 = xgb_opt3.predict_proba(x_test_3)[:,1]\n",
    "\n",
    "# getting optimal cutoff\n",
    "opt_cutoff3 = cost_function_cutoff(y_test_3, xgb_pred3)\n",
    "\n",
    "# likilyhoods to labels\n",
    "xgb_label3 = np.where(xgb_pred3 < opt_cutoff3, 0, 1)\n",
    "\n",
    "# scoring the model \n",
    "con_mat = confusion_matrix(y_test_3, xgb_label3)\n",
    "print(con_mat)\n",
    "print('the cost of the xgb model is: ', -25 * con_mat[1, 0] - 5 * con_mat[0, 1] + 5 * con_mat[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ab9dcfb2-db74-4d71-8931-fff979c1356a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rf model:  {'max_depth': 7, 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "print('best rf model: ', rf_grid3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "93c9bc06-8816-4818-b13a-08c389fd00ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best gb model:  {'n_estimators': 300, 'min_samples_split': 15, 'min_samples_leaf': 7, 'max_depth': 3, 'learning_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "print('best gb model: ', gb_grid3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a18b86cf-a3cf-4b65-b060-22c7fa1f9d2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best xgb model:  {'n_estimators': 323, 'max_depth': 5, 'min_child_weight': 5, 'learning_rate': 0.08199481025626429, 'gamma': 0.18013848692542023, 'subsample': 0.9112569903715746}\n"
     ]
    }
   ],
   "source": [
    "print('best xgb model: ', study3.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10882f4b-0ea4-452b-99fd-2be50ae9d5dd",
   "metadata": {},
   "source": [
    "# Meta Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a00f86ab-e125-40bd-a42a-b6aa498f17ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defining imput and target\n",
    "x = train[['trustLevel', 'totalScanTimeInSeconds', 'interaction_1', 'heredity_1', 'heredity_2']]\n",
    "y = train['fraud']\n",
    "\n",
    "test = test[['trustLevel', 'totalScanTimeInSeconds', 'interaction_1', 'heredity_1', 'heredity_2']]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.2, stratify = y)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, test_size = 0.5, stratify = y_val)\n",
    "\n",
    "# running model with best parameters\n",
    "rf_md = RandomForestClassifier(max_depth = 7, min_samples_leaf = 5, min_samples_split = 10, n_estimators = 100).fit(x_train, y_train)\n",
    "rf_val_pred = rf_md.predict_proba(x_val)[:,1]\n",
    "rf_test_pred = rf_md.predict_proba(x_test)[:,1]\n",
    "\n",
    "gb_md = GradientBoostingClassifier(n_estimators = 300, min_samples_split = 15, min_samples_leaf = 7, max_depth = 3, learning_rate = 0.01).fit(x_train, y_train)\n",
    "gb_val_pred = gb_md.predict_proba(x_val)[:,1]\n",
    "gb_test_pred = gb_md.predict_proba(x_test)[:,1]\n",
    "\n",
    "xgb_md = XGBClassifier(n_estimators = 323, max_depth = 5, min_child_weight = 5, learning_rate = 0.08199481025626429, gamma = 0.18013848692542023, subsample = 0.9112569903715746).fit(x_train, y_train)\n",
    "xgb_val_pred = xgb_md.predict_proba(x_val)[:,1]\n",
    "xgb_test_pred = xgb_md.predict_proba(x_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5787287d-0386-4619-9042-23288d7ef59d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[178   0]\n",
      " [ 10   0]]\n",
      "the cost of the meta model is:  -250\n"
     ]
    }
   ],
   "source": [
    "# creating predictions dataframe\n",
    "x_preds = pd.DataFrame({'rf': rf_val_pred, 'gb': gb_val_pred, 'xgb': xgb_val_pred})\n",
    "x_test_preds = pd.DataFrame({'rf': rf_test_pred, 'gb': gb_test_pred, 'xgb': xgb_test_pred})\n",
    "\n",
    "XGB_param_grid = {'n_estimators': [300, 500],\n",
    "                    'max_depth': [3, 7],\n",
    "                     'min_child_weight': [5, 7],\n",
    "                     'learning_rate': [0.01, 0.1],\n",
    "                     'gamma': [0.1, 0.3],\n",
    "                     'subsample': [0.8, 1]}\n",
    "\n",
    "# running grid search \n",
    "xgb_meta_grid = RandomizedSearchCV(estimator = XGBClassifier(), \n",
    "                               param_distributions = XGB_param_grid, cv = 3, scoring = my_scorer_function, n_jobs = -1).fit(x_preds, y_val)\n",
    "\n",
    "# running model with best parameters\n",
    "meta_opt = XGBClassifier(**xgb_meta_grid.best_params_).fit(x_preds, y_val)\n",
    "\n",
    "# predicting on test\n",
    "meta_pred = meta_opt.predict_proba(x_test_preds)[:,1]\n",
    "\n",
    "# getting optimal cutoff\n",
    "opt_cutoff = cost_function_cutoff(y_val, meta_pred)\n",
    "\n",
    "# likilyhoods to labels\n",
    "meta_label = np.where(meta_pred < opt_cutoff, 0, 1)\n",
    "\n",
    "# scoring the model \n",
    "con_mat = confusion_matrix(y_val, meta_label)\n",
    "print(con_mat)\n",
    "print('the cost of the meta model is: ', -25 * con_mat[1, 0] - 5 * con_mat[0, 1] + 5 * con_mat[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84601ba3-3592-4951-9dc8-213881af74b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Cutoff:  0.51\n"
     ]
    }
   ],
   "source": [
    "print('Optimal Cutoff: ', opt_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6eab2dd5-404f-4d4f-aabf-a5ee07d79e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(meta_pred).to_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p38",
   "language": "python",
   "name": "conda_mxnet_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
